# Ollama Performance Configuration
# Copy this file to .env and modify as needed

# Keep model loaded in memory indefinitely (-1) or specify duration (e.g., "30m", "1h")
OLLAMA_KEEP_ALIVE=-1

# Model generation parameters for performance optimization
OLLAMA_TEMPERATURE=0.3          # Lower = more focused responses (0.0 to 1.0)
OLLAMA_TOP_P=0.8               # Top-p sampling for token selection (0.0 to 1.0)
OLLAMA_TOP_K=40                # Top-k sampling - consider top K tokens (1 to 100)
OLLAMA_NUM_PREDICT=512         # Maximum tokens to generate per response
OLLAMA_CONTEXT_SIZE=4096       # Context window size (must be consistent)

# Alternative settings for different use cases:

# For fastest responses (less creative, shorter):
# OLLAMA_TEMPERATURE=0.1
# OLLAMA_NUM_PREDICT=256
# OLLAMA_TOP_K=20

# For more creative responses (slower):
# OLLAMA_TEMPERATURE=0.7
# OLLAMA_NUM_PREDICT=1024
# OLLAMA_TOP_K=80

# For development/testing with smaller models:
# Consider using: phi3:mini, gemma2:2b, or qwen2.5:1.5b
# These are faster but less capable than gemma3:4b

# Device Monitoring Credentials (Optional - can be set per device)
# Use device ID in uppercase with _PASSWORD, _API_TOKEN, or _API_KEY suffix
# Examples:
# CORE-ROUTER_PASSWORD=your_router_password
# CORE-SWITCH_API_TOKEN=your_switch_api_token
# FIREWALL_API_KEY=your_firewall_api_key 